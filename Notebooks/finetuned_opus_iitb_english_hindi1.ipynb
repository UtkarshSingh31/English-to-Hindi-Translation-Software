{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from sacrebleu import corpus_bleu, corpus_chrf\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce GTX 1650\n",
            "PyTorch version: 2.7.1+cu118\n"
          ]
        }
      ],
      "source": [
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Split: 3653 train, 914 val\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# STEP 1: Load Data (SHUFFLE!)\n",
        "# ============================================\n",
        "df = pd.read_csv(\"E:\\\\English-to-Hindi-Translation-Software\\\\data\\\\eng_hind.csv\").dropna()\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "train_df = df[:train_size]\n",
        "val_df = df[train_size:]\n",
        "\n",
        "train_df.to_csv(\"E:\\\\English-to-Hindi-Translation-Software\\\\data\\\\train.csv\", index=False)\n",
        "val_df.to_csv(\"E:\\\\English-to-Hindi-Translation-Software\\\\data\\\\val.csv\", index=False)\n",
        "print(f\"‚úÖ Split: {len(train_df)} train, {len(val_df)} val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 2: Simple Dataset (NO FANCY STUFF)\n",
        "# ============================================\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, csv_path, tokenizer, max_length=96):  # ‚Üê 96 is sweet spot\n",
        "        df = pd.read_csv(csv_path).dropna()\n",
        "        self.eng = df[\"English\"].tolist()\n",
        "        self.hin = df[\"Hindi\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.eng)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src = self.eng[index]\n",
        "        tgt = self.hin[index]\n",
        "\n",
        "        # Standard encoding\n",
        "        src_enc = self.tokenizer(\n",
        "            src,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # CRITICAL: Use text_target for MarianMT!\n",
        "        tgt_enc = self.tokenizer(\n",
        "            text_target=tgt,  # ‚Üê This is KEY for Hindi!\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": src_enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": src_enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": tgt_enc[\"input_ids\"].squeeze(0)\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\English-to-Hindi-Translation-Software\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ready: 914 train batches\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# STEP 3: Setup (EXACT SETTINGS)\n",
        "# ============================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Datasets with max_length=96 (not 64, not 128)\n",
        "train_dataset = TranslationDataset(\"E:\\\\English-to-Hindi-Translation-Software\\\\data\\\\train.csv\", tokenizer, max_length=96)\n",
        "val_dataset = TranslationDataset(\"E:\\\\English-to-Hindi-Translation-Software\\\\data\\\\val.csv\", tokenizer, max_length=96)\n",
        "\n",
        "# DataLoaders (smaller batch = better quality)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # ‚Üê 4, not 8!\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "print(f\"‚úÖ Ready: {len(train_loader)} train batches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 12 epochs.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# STEP 4: OPTIMAL Training Settings\n",
        "# ============================================\n",
        "# CRITICAL: Lower learning rate for fine-tuning\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-5,  # ‚Üê Lower than before (was 5e-5)\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Add warmup + linear decay (prevents catastrophic forgetting)\n",
        "num_epochs = 12  # ‚Üê Sweet spot for 4.5k dataset\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "num_warmup_steps = len(train_loader) * 2  # ‚Üê 2 epochs warmup\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience = 3  # Early stopping\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"Training for {num_epochs} epochs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [03:59<00:00,  3.81it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train: 1.5873 | Val: 0.4532 | LR: 1.50e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\English-to-Hindi-Translation-Software\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model saved! (Val Loss: 0.4532)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:43<00:00,  5.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train: 0.3984 | Val: 0.3660 | LR: 3.00e-05\n",
            "Best model saved! (Val Loss: 0.3660)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:43<00:00,  5.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train: 0.2798 | Val: 0.3289 | LR: 2.70e-05\n",
            "Best model saved! (Val Loss: 0.3289)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:43<00:00,  5.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train: 0.2005 | Val: 0.3142 | LR: 2.40e-05\n",
            "Best model saved! (Val Loss: 0.3142)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:43<00:00,  5.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train: 0.1488 | Val: 0.3110 | LR: 2.10e-05\n",
            "Best model saved! (Val Loss: 0.3110)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:42<00:00,  5.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Train: 0.1134 | Val: 0.3067 | LR: 1.80e-05\n",
            "Best model saved! (Val Loss: 0.3067)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:42<00:00,  5.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Train: 0.0882 | Val: 0.3058 | LR: 1.50e-05\n",
            "Best model saved! (Val Loss: 0.3058)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:42<00:00,  5.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Train: 0.0705 | Val: 0.3067 | LR: 1.20e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:42<00:00,  5.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Train: 0.0579 | Val: 0.3090 | LR: 9.00e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [02:42<00:00,  5.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Train: 0.0485 | Val: 0.3093 | LR: 6.00e-06\n",
            "\n",
            "Early stopping at epoch 10 (no improvement for 3 epochs)\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# STEP 5: Training Loop with Early Stopping\n",
        "# ============================================\n",
        "for epoch in range(num_epochs):\n",
        "    # === TRAINING ===\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (prevents explosion)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # ‚Üê Update learning rate\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # === VALIDATION ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f} | LR: {current_lr:.2e}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "\n",
        "        model.save_pretrained(\"./models/best_helsinki_model\")\n",
        "        tokenizer.save_pretrained(\"./models/best_helsinki_model\")\n",
        "        print(f\"Best model saved! (Val Loss: {avg_val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 6: Inference with Proper Settings\n",
        "# ============================================\n",
        "def translate(text):\n",
        "    \"\"\"Fixed inference matching training settings\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode with same max_length as training\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=96,  # ‚Üê Match training!\n",
        "            truncation=True,\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate with proper settings\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=96,      # ‚Üê Match training!\n",
        "            num_beams=4,        # ‚Üê Lower beams = less repetition\n",
        "            length_penalty=1.0,  # ‚Üê Balanced length\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2,  # ‚Üê Prevent loops\n",
        "            repetition_penalty=1.2   # ‚Üê Avoid repetition\n",
        "        )\n",
        "\n",
        "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # F3: Fix section numbers\n",
        "        sections = re.findall(r'Section\\s+(\\d+)', text, re.IGNORECASE)\n",
        "        if sections:\n",
        "            translation = re.sub(r'‡§ß‡§æ‡§∞‡§æ\\s+\\d+', f'‡§ß‡§æ‡§∞‡§æ {sections[0]}', translation)\n",
        "\n",
        "        return translation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUMpHZ4RMbYf",
        "outputId": "4e4c4b6f-7727-4177-b21a-762b1a69b404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING BEST MODEL\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\English-to-Hindi-Translation-Software\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EN: Parking is not allowed.\n",
            "HI: ‡§™‡§æ‡§∞‡•ç‡§ï‡§ø‡§Ç‡§ó ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§\n",
            "\n",
            "EN: Unauthorized entry is prohibited.\n",
            "HI: ‡§Ö‡§®‡§ß‡§ø‡§ï‡•É‡§§ ‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§π‡•à‡•§\n",
            "\n",
            "EN: This notice is issued under Section 144 of the Criminal Procedure Code\n",
            "HI: ‡§Ø‡§π ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§¶‡§Ç‡§° ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§∏‡§Ç‡§π‡§ø‡§§‡§æ ‡§ï‡•Ä ‡§ß‡§æ‡§∞‡§æ 144 ‡§ï‡•á ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§ó‡§§ ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§ó‡§à ‡§π‡•à\n",
            "\n",
            "EN: The authorities announced a new curfew order.\n",
            "HI: ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§®‡•á ‡§®‡§Ø‡§æ ‡§ï‡§∞‡•ç‡§´‡•ç‡§Ø‡•Ç ‡§Ü‡§¶‡•á‡§∂ ‡§ò‡•ã‡§∑‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ‡•§\n",
            "\n",
            "EN: The policy will be implemented immediately.\n",
            "HI: ‡§®‡•Ä‡§§‡§ø ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡•Ä ‡§ú‡§æ‡§è‡§ó‡•Ä‡•§\n",
            "\n",
            "EN: Smoking is strictly forbidden in public places.\n",
            "HI: ‡§∏‡§æ‡§∞‡•ç‡§µ‡§ú‡§®‡§ø‡§ï ‡§∏‡•ç‡§•‡§æ‡§®‡•ã‡§Ç ‡§™‡§∞ ‡§ß‡•Ç‡§Æ‡•ç‡§∞‡§™‡§æ‡§® ‡§∏‡§ñ‡•ç‡§§‡•Ä ‡§∏‡•á ‡§µ‡§∞‡•ç‡§ú‡§ø‡§§ ‡§π‡•à‡•§\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 7: Test Translations\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING BEST MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load best model\n",
        "model = MarianMTModel.from_pretrained(\"./models/best_helsinki_model\").to(device)\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"./models/best_helsinki_model\")\n",
        "\n",
        "test_cases = [\n",
        "    \"Parking is not allowed.\",\n",
        "    \"Unauthorized entry is prohibited.\",\n",
        "    \"This notice is issued under Section 144 of the Criminal Procedure Code\",\n",
        "    \"The authorities announced a new curfew order.\",\n",
        "    \"The policy will be implemented immediately.\",\n",
        "    \"Smoking is strictly forbidden in public places.\"\n",
        "]\n",
        "\n",
        "for text in test_cases:\n",
        "    result = translate(text)\n",
        "    print(f\"\\nEN: {text}\")\n",
        "    print(f\"HI: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qc3kqDCwlBq",
        "outputId": "b8dd30f6-0fa1-437e-d60c-2e97aaafa14f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on 914 validation samples...\n",
            "This may take a few minutes...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  11%|‚ñà         | 100/914 [00:35<04:58,  2.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 100/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  22%|‚ñà‚ñà‚ñè       | 200/914 [01:08<04:07,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 200/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  33%|‚ñà‚ñà‚ñà‚ñé      | 300/914 [01:40<03:07,  3.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 300/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 400/914 [02:15<02:15,  3.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 400/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 500/914 [02:47<02:09,  3.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 500/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 600/914 [03:21<02:16,  2.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 600/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 700/914 [03:53<01:21,  2.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 700/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 801/914 [04:27<00:36,  3.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 800/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 901/914 [05:00<00:03,  3.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 900/914 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating translations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [05:04<00:00,  3.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üìä FINAL EVALUATION RESULTS\n",
            "============================================================\n",
            "‚úÖ BLEU Score:  41.76\n",
            "‚úÖ chrF Score:  64.91\n",
            "‚úÖ Precision:   69.12% (1-gram)\n",
            "‚úÖ Precision:   49.18% (2-gram)\n",
            "‚úÖ Precision:   35.97% (3-gram)\n",
            "‚úÖ Precision:   27.16% (4-gram)\n",
            "============================================================\n",
            "\n",
            "üìã QUALITATIVE ANALYSIS (Sample Translations):\n",
            "\n",
            "Example 655:\n",
            "  Source:      Diversion signs have been installed for commuters\n",
            "  Reference:   ‡§Ø‡§æ‡§§‡•ç‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§æ‡§Ø‡§µ‡§∞‡•ç‡§ú‡§® ‡§∏‡§Ç‡§ï‡•á‡§§ ‡§≤‡§ó‡§æ‡§è ‡§ó‡§è ‡§π‡•à‡§Ç\n",
            "  Prediction:  ‡§Ø‡§æ‡§§‡•ç‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∞‡•Ç‡§™‡§æ‡§Ç‡§§‡§∞‡§£ ‡§∏‡§Ç‡§ï‡•á‡§§ ‡§≤‡§ó‡§æ‡§è ‡§ó‡§è ‡§π‡•à‡§Ç\n",
            "\n",
            "Example 115:\n",
            "  Source:      Original documents must be produced for comparison when required by the court\n",
            "  Reference:   ‡§Æ‡•Ç‡§≤ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§≤‡§Ø ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§§‡•Å‡§≤‡§®‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡•Å‡§§ ‡§ï‡§∞‡§®‡•á ‡§π‡•ã‡§Ç‡§ó‡•á\n",
            "  Prediction:  ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§≤‡§Ø ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§Æ‡•Ç‡§≤ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡•ã‡§Ç ‡§ï‡•Ä ‡§§‡•Å‡§≤‡§®‡§æ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è\n",
            "\n",
            "Example 26:\n",
            "  Source:      Jan Dhan account holders get free RuPay debit card and accident insurance. Zero balance accounts allowed.\n",
            "  Reference:   ‡§ú‡§® ‡§ß‡§® ‡§ñ‡§æ‡§§‡§æ‡§ß‡§æ‡§∞‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§ø‡§É‡§∂‡•Å‡§≤‡•ç‡§ï ‡§∞‡•Å‡§™‡•á ‡§°‡•á‡§¨‡§ø‡§ü ‡§ï‡§æ‡§∞‡•ç‡§° ‡§è‡§µ‡§Ç ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§¨‡•Ä‡§Æ‡§æ ‡§Æ‡§ø‡§≤‡•á‡§ó‡§æ‡•§ ‡§∂‡•Ç‡§®‡•ç‡§Ø ‡§¨‡•à‡§≤‡•á‡§Ç‡§∏ ‡§ñ‡§æ‡§§‡•á ‡§Ö‡§®‡•Å‡§Æ‡§§ ‡§π‡•à‡§Ç‡•§\n",
            "  Prediction:  ‡§ú‡§®‡§® ‡§ñ‡§æ‡§§‡§æ ‡§ß‡§æ‡§∞‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§ø‡§É‡§∂‡•Å‡§≤‡•ç‡§ï ‡§∞‡•Ç‡§ü‡•â‡§™ ‡§ï‡§æ‡§∞‡•ç‡§° ‡§î‡§∞ ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§¨‡•Ä‡§Æ‡§æ‡•§ ‡§∂‡•Ç‡§®‡•ç‡§Ø ‡§∏‡§Ç‡§§‡•Å‡§≤‡§® ‡§ñ‡§æ‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§π‡•à‡•§\n",
            "\n",
            "Example 760:\n",
            "  Source:      Heritage Walk starts daily at 7 AM from monument entrance. Comfortable shoes and water bottles recommended for participants.\n",
            "  Reference:   ‡§ß‡§∞‡•ã‡§π‡§∞ ‡§≠‡•ç‡§∞‡§Æ‡§£ ‡§™‡•ç‡§∞‡§§‡§ø‡§¶‡§ø‡§® ‡§∏‡•Å‡§¨‡§π 7 ‡§¨‡§ú‡•á ‡§∏‡•ç‡§Æ‡§æ‡§∞‡§ï ‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§¶‡•ç‡§µ‡§æ‡§∞ ‡§∏‡•á‡•§ ‡§™‡•ç‡§∞‡§§‡§ø‡§≠‡§æ‡§ó‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§∞‡§æ‡§Æ‡§¶‡§æ‡§Ø‡§ï ‡§ú‡•Ç‡§§‡•á ‡§è‡§µ‡§Ç ‡§™‡§æ‡§®‡•Ä ‡§ï‡•Ä ‡§¨‡•ã‡§§‡§≤ ‡§∏‡§æ‡§• ‡§≤‡§æ‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è‡•§\n",
            "  Prediction:  ‡§µ‡§ø‡§∞‡§æ‡§∏‡§§ ‡§è‡§ú‡•á‡§Ç‡§∏‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§¶‡§ø‡§® ‡§∏‡•ç‡§Æ‡§æ‡§∞‡§ï ‡§¶‡•ç‡§µ‡§æ‡§∞ ‡§∏‡•á 7 ‡§¨‡§ú‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§ ‡§∏‡§π‡§ï‡§æ‡§∞‡•Ä ‡§ú‡•Ç‡§§‡•á ‡§î‡§∞ ‡§™‡§æ‡§®‡•Ä ‡§ï‡•Ä ‡§¨‡•ã‡§§‡§≤‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∂ ‡§ï‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à\n",
            "\n",
            "Example 282:\n",
            "  Source:      The authority will conduct field inspections\n",
            "  Reference:   ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•Ä‡§Ø ‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§ï‡§∞‡•á‡§ó‡§æ\n",
            "  Prediction:  ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£ ‡§´‡•Ä‡§≤‡•ç‡§° ‡§®‡§ø‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§ï‡§∞‡•á‡§ó‡§æ\n",
            "\n",
            "Example 251:\n",
            "  Source:      New book issue limit applied from this semester.\n",
            "  Reference:   ‡§á‡§∏ ‡§∏‡§§‡•ç‡§∞ ‡§∏‡•á ‡§®‡§à ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§ú‡§æ‡§∞‡•Ä ‡§∏‡•Ä‡§Æ‡§æ ‡§≤‡§æ‡§ó‡•Ç‡•§\n",
            "  Prediction:  ‡§á‡§∏ ‡§∏‡•á‡§Æ‡•á‡§∏‡•ç‡§ü‡§∞ ‡§∏‡•á ‡§®‡§à ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§ó‡§à ‡§π‡•à‡•§\n",
            "\n",
            "Example 229:\n",
            "  Source:      Bluetooth must be switched off\n",
            "  Reference:   ‡§¨‡•ç‡§≤‡•Ç‡§ü‡•Ç‡§• ‡§¨‡§Ç‡§¶ ‡§∞‡§ñ‡§®‡§æ ‡§Ö‡§®‡§ø‡§µ‡§æ‡§∞‡•ç‡§Ø ‡§π‡•à\n",
            "  Prediction:  ‡§¨‡•ç‡§≤‡•Ç‡§ü‡•Ç‡§• ‡§¨‡§Ç‡§¶ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è\n",
            "\n",
            "Example 143:\n",
            "  Source:      Pradhan Mantri Bhartiya Janaushadhi Kendras at 1000+ stations. Generic medicines 50-90% cheaper.\n",
            "  Reference:   ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§ú‡§®‡§î‡§∑‡§ß‡§ø ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞ 1000+ ‡§∏‡•ç‡§ü‡•á‡§∂‡§®‡•ã‡§Ç ‡§™‡§∞‡•§ ‡§ú‡•á‡§®‡•á‡§∞‡§ø‡§ï ‡§¶‡§µ‡§æ‡§è‡§Ç 50-90% ‡§∏‡§∏‡•ç‡§§‡•Ä‡•§\n",
            "  Prediction:  ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§≠‡§æ‡§∞‡§§ ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞ 1000+ ‡§∏‡•ç‡§ü‡•á‡§∂‡§®‡•ã‡§Ç ‡§™‡§∞‡•§ ‡§ú‡•á‡§®‡•á‡§∞‡§ø‡§ï ‡§¶‡§µ‡§æ 50- 90% ‡§™‡•à‡§ï‡•á‡§ú‡§ø‡§Ç‡§ó‡•§\n",
            "\n",
            "Example 755:\n",
            "  Source:      All departments must conduct at least two parent-teacher meetings per semester and maintain attendance records.\n",
            "  Reference:   ‡§∏‡§≠‡•Ä ‡§µ‡§ø‡§≠‡§æ‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§™‡•ç‡§∞‡§§‡§ø ‡§∏‡•á‡§Æ‡•á‡§∏‡•ç‡§ü‡§∞ ‡§ï‡§Æ ‡§∏‡•á ‡§ï‡§Æ ‡§¶‡•ã ‡§Ö‡§≠‡§ø‡§≠‡§æ‡§µ‡§ï-‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§¨‡•à‡§†‡§ï‡•á‡§Ç ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§ï‡§∞‡§®‡•Ä ‡§π‡•ã‡§Ç‡§ó‡•Ä ‡§î‡§∞ ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§¨‡§®‡§æ‡§è ‡§∞‡§ñ‡§®‡•á ‡§π‡•ã‡§Ç‡§ó‡•á‡•§\n",
            "  Prediction:  ‡§∏‡§≠‡•Ä ‡§µ‡§ø‡§≠‡§æ‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§∏‡•á‡§Æ‡•á‡§∏‡•ç‡§ü‡§∞ ‡§™‡•ç‡§∞‡§§‡§ø ‡§ï‡§Æ ‡§∏‡•á ‡§ï‡§Æ ‡§¶‡•ã ‡§Ö‡§≠‡§ø‡§≠‡§æ‡§µ‡§ï-‡§∂‡•Å‡§≤‡•ç‡§ï ‡§¨‡•à‡§†‡§ï ‡§∏‡§Ç‡§ö‡§æ‡§≤‡§ø‡§§ ‡§ï‡§∞‡§®‡•Ä ‡§π‡•ã‡§ó‡•Ä ‡§î‡§∞ ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Ö‡§≠‡§ø‡§≤‡•á‡§ñ ‡§¨‡§®‡§æ‡§è ‡§∞‡§ñ‡§®‡§æ ‡§π‡•ã‡§ó‡§æ‡•§\n",
            "\n",
            "Example 105:\n",
            "  Source:      The authority will review applications objectively\n",
            "  Reference:   ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£ ‡§Ü‡§µ‡•á‡§¶‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§®‡§ø‡§∑‡•ç‡§™‡§ï‡•ç‡§∑ ‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡§∞‡•á‡§ó‡§æ\n",
            "  Prediction:  ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£ ‡§Ü‡§µ‡•á‡§¶‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡•ç‡§Ø ‡§∏‡•á ‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡§∞‡•á‡§ó‡§æ\n",
            "\n",
            "============================================================\n",
            "üìù F1: FORMALITY ANALYSIS\n",
            "============================================================\n",
            "‚úÖ Formal style usage: 10.8%\n",
            "   (99/914 translations contain formal keywords)\n",
            "\n",
            "============================================================\n",
            "üî¢ F3: NUMBER PRESERVATION ANALYSIS\n",
            "============================================================\n",
            "‚úÖ Number preservation accuracy: 81.2%\n",
            "   (104/128 correctly preserved)\n",
            "   Errors: 24\n",
            "\n",
            "============================================================\n",
            "üìñ F2: TERMINOLOGY CONSISTENCY\n",
            "============================================================\n",
            "‚úÖ Terminology consistency: 92.0%\n",
            "   (115/125 terms correctly translated)\n",
            "\n",
            "============================================================\n",
            "üíæ SAVING DETAILED RESULTS\n",
            "============================================================\n",
            "‚úÖ Saved detailed results to: evaluation_results.csv\n",
            "\n",
            "============================================================\n",
            "üìà SUMMARY STATISTICS\n",
            "============================================================\n",
            "Total validation samples: 914\n",
            "Average source length: 11.4 words\n",
            "Average reference length: 13.4 words\n",
            "Average prediction length: 13.1 words\n",
            "\n",
            "============================================================\n",
            "üéØ FINAL VERDICT\n",
            "============================================================\n",
            "üèÜ EXCELLENT! Production-ready quality!\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Load validation data\n",
        "val_data = pd.read_csv(\"E:\\\\English-to-Hindi-Translation-Software\\\\data\\\\val.csv\").dropna()\n",
        "sources = val_data['English'].tolist()\n",
        "references = val_data['Hindi'].tolist()\n",
        "\n",
        "print(f\"\\nEvaluating on {len(sources)} validation samples...\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "# Generate predictions for ALL validation data\n",
        "predictions = []\n",
        "for i, src in enumerate(tqdm(sources, desc=\"Generating translations\")):\n",
        "    pred = translate(src)\n",
        "    predictions.append(pred)\n",
        "\n",
        "    # Show progress every 100 samples\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"  Processed {i+1}/{len(sources)} samples\")\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu = corpus_bleu(predictions, [references])\n",
        "chrf = corpus_chrf(predictions, [references])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ BLEU Score:  {bleu.score:.2f}\")\n",
        "print(f\"‚úÖ chrF Score:  {chrf.score:.2f}\")\n",
        "print(f\"‚úÖ Precision:   {bleu.precisions[0]:.2f}% (1-gram)\")\n",
        "print(f\"‚úÖ Precision:   {bleu.precisions[1]:.2f}% (2-gram)\")\n",
        "print(f\"‚úÖ Precision:   {bleu.precisions[2]:.2f}% (3-gram)\")\n",
        "print(f\"‚úÖ Precision:   {bleu.precisions[3]:.2f}% (4-gram)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Additional analysis\n",
        "print(\"\\nüìã QUALITATIVE ANALYSIS (Sample Translations):\\n\")\n",
        "\n",
        "# Show 10 random samples\n",
        "import random\n",
        "random.seed(42)\n",
        "sample_indices = random.sample(range(len(sources)), min(10, len(sources)))\n",
        "\n",
        "for idx in sample_indices:\n",
        "    print(f\"Example {idx+1}:\")\n",
        "    print(f\"  Source:      {sources[idx]}\")\n",
        "    print(f\"  Reference:   {references[idx]}\")\n",
        "    print(f\"  Prediction:  {predictions[idx]}\")\n",
        "    print()\n",
        "\n",
        "# F1: Formality Analysis\n",
        "print(\"=\"*60)\n",
        "print(\"üìù F1: FORMALITY ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "formal_keywords = ['‡§µ‡§∞‡•ç‡§ú‡§ø‡§§', '‡§®‡§ø‡§∑‡§ø‡§¶‡•ç‡§ß', '‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£', '‡§∏‡•Ç‡§ö‡§®‡§æ', '‡§ß‡§æ‡§∞‡§æ', '‡§Ö‡§ß‡§ø‡§∏‡•Ç‡§ö‡§®‡§æ', '‡§§‡§§‡•ç‡§ï‡§æ‡§≤']\n",
        "formal_count = 0\n",
        "\n",
        "for pred in predictions:\n",
        "    if any(kw in pred for kw in formal_keywords):\n",
        "        formal_count += 1\n",
        "\n",
        "formality_pct = (formal_count / len(predictions)) * 100\n",
        "print(f\"‚úÖ Formal style usage: {formality_pct:.1f}%\")\n",
        "print(f\"   ({formal_count}/{len(predictions)} translations contain formal keywords)\")\n",
        "\n",
        "# F3: Number Preservation Analysis\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî¢ F3: NUMBER PRESERVATION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "number_errors = 0\n",
        "total_with_numbers = 0\n",
        "\n",
        "for src, pred in zip(sources, predictions):\n",
        "    src_numbers = set(re.findall(r'\\b\\d+\\b', src))\n",
        "    if src_numbers:\n",
        "        total_with_numbers += 1\n",
        "        pred_numbers = set(re.findall(r'\\b\\d+\\b', pred))\n",
        "        if src_numbers != pred_numbers:\n",
        "            number_errors += 1\n",
        "\n",
        "if total_with_numbers > 0:\n",
        "    number_accuracy = ((total_with_numbers - number_errors) / total_with_numbers) * 100\n",
        "    print(f\"‚úÖ Number preservation accuracy: {number_accuracy:.1f}%\")\n",
        "    print(f\"   ({total_with_numbers - number_errors}/{total_with_numbers} correctly preserved)\")\n",
        "    print(f\"   Errors: {number_errors}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No numbers found in validation set\")\n",
        "\n",
        "# Terminology Consistency Check\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìñ F2: TERMINOLOGY CONSISTENCY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "term_mapping = {\n",
        "    'prohibited': ['‡§µ‡§∞‡•ç‡§ú‡§ø‡§§', '‡§®‡§ø‡§∑‡§ø‡§¶‡•ç‡§ß', '‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§Ç‡§ß‡§ø‡§§'],\n",
        "    'notice': ['‡§∏‡•Ç‡§ö‡§®‡§æ', '‡§®‡•ã‡§ü‡§ø‡§∏'],\n",
        "    'section': ['‡§ß‡§æ‡§∞‡§æ'],\n",
        "    'authority': ['‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£', '‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä'],\n",
        "    'immediately': ['‡§§‡•Å‡§∞‡§Ç‡§§', '‡§§‡§§‡•ç‡§ï‡§æ‡§≤']\n",
        "}\n",
        "\n",
        "consistency_count = 0\n",
        "total_terms = 0\n",
        "\n",
        "for src, pred in zip(sources, predictions):\n",
        "    for eng_term, hindi_options in term_mapping.items():\n",
        "        if eng_term in src.lower():\n",
        "            total_terms += 1\n",
        "            if any(hin in pred for hin in hindi_options):\n",
        "                consistency_count += 1\n",
        "\n",
        "if total_terms > 0:\n",
        "    consistency_pct = (consistency_count / total_terms) * 100\n",
        "    print(f\"‚úÖ Terminology consistency: {consistency_pct:.1f}%\")\n",
        "    print(f\"   ({consistency_count}/{total_terms} terms correctly translated)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No key terms found in validation set\")\n",
        "\n",
        "# Save detailed results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üíæ SAVING DETAILED RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'English': sources,\n",
        "    'Reference_Hindi': references,\n",
        "    'Predicted_Hindi': predictions\n",
        "})\n",
        "\n",
        "results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
        "print(\"‚úÖ Saved detailed results to: evaluation_results.csv\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà SUMMARY STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total validation samples: {len(sources)}\")\n",
        "print(f\"Average source length: {sum(len(s.split()) for s in sources)/len(sources):.1f} words\")\n",
        "print(f\"Average reference length: {sum(len(r.split()) for r in references)/len(references):.1f} words\")\n",
        "print(f\"Average prediction length: {sum(len(p.split()) for p in predictions)/len(predictions):.1f} words\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ FINAL VERDICT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if bleu.score >= 35:\n",
        "    print(\"üèÜ EXCELLENT! Production-ready quality!\")\n",
        "elif bleu.score >= 25:\n",
        "    print(\"‚úÖ GOOD! Suitable for deployment with monitoring.\")\n",
        "elif bleu.score >= 15:\n",
        "    print(\"‚ö†Ô∏è FAIR. Consider more training or data.\")\n",
        "else:\n",
        "    print(\"‚ùå POOR. Needs significant improvement.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dElio3ES3Fhu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
